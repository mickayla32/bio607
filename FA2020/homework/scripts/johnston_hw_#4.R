##############
#
# Homework Week 4: Fucntions and Pivoting
#
# 2020-10-05
#
##############

# Libraries
library(dplyr)
library(purrr)
library(tidyr)
library(stringr)
library(ggplot2)

# 1. Write a function that takes a vector and returns one bootstrapped sample from said vector. Demonstrate that it works.

# Write the function:
boot_samp <- function(vec){
  
  one_boot <- sample(vec,
                     size = length(vec),
                     replace = TRUE)
  
  return(one_boot)
  
}

# Show that it works:

  # set seed
set.seed(123)
  # Create a test vector
vec <-rnorm(32, mean = 10, sd = 2)

  # Run the function
boot_samp(vec)

# 2. Write a function that, given a vector of values, a request for some number of bootstraps (let’s call the parameter R), and a sample statistic function (e.g., mean, IQR, etc.) 
  # returns R number of values of that statistic. Have it default to R = 1000 
  # and the function is mean. 
  # Show this works for 10 bootstrapped replicate draws of a mean from some vector. 
  # Do the values look reasonable? Compare to the actual mean of the vector.
  # make sure you are using the function(s) you wrote in #1

# Write a function that, given a vector, some # of bootstraps (r = 1000), and a sample statistic function (mean)
boot_means_fun <- function(vec, r = 1000, fun = mean){

boot_means <- replicate(r,
                        boot_samp(vec) %>%
                        mean())
# returns r # of means
return(boot_means)
   
}


# Test it
# (using same vec from previously)
boot_means_fun(vec = vec, r = 10)

# Compare these values to the actual mean of the vector:
mean(vec)
# Yes, these values are relatively close to the vector mean, which is 9.919893

# 3. Write a function that, given a vector of values, a request for some number of bootstraps, and a sample statistic function, 
  # returns the original value of the statistic as applied to the vector, 
  # the mean of the statistic generated by the bootstrapped reps, 
  # the upper and lower 95% CI of the bootstrapped statistic (e.g., the 0.025 and 0.975 quantile), 
  # and the bias (i.e., the original value of the statistic - the mean of the bootstrapped statistic).
  # make sure you are using the function(s) you wrote in #1 and/or #2

# Write a function that, given a vector of values, a request for some number of bootstraps, and a sample statistic function, 

new_fun <- function(vec, r = 1000, fun = mean){
  
  # returns the original value of the statistic as applied to the vector, 
  true_mean <- mean(vec)
  
  #return(true_mean)
  
  # the mean of the statistic generated by the bootstrapped reps, 
  boot_samps <- replicate(r,
                          boot_samp(vec) %>%
                            mean())
  
  boot_mean <- mean(boot_samps)
  
  #return(boot_mean)
  
  # the upper and lower 95% CI of the bootstrapped statistic (e.g., the 0.025 and 0.975 quantile)
  
  upper_quant <- quantile(boot_mean, probs = 0.975)
  lower_quant <- quantile(boot_mean, probs = 0.025)
  
  #return(c(lower_quant, upper_quant))
  
  
  # and the bias (i.e., the original value of the statistic - the mean of the bootstrapped statistic).
  
  bias <- (true_mean - boot_mean)
  
  #return(bias)
  
  # Return everything in a df
  out <- data.frame(true_mean = true_mean,
                    boot_mean = boot_mean,
                    lower_quant = lower_quant,
                    upper_quant = upper_quant,
                    bias = bias)
  
  return(out)
}

# Show it works:
new_fun(vec, r= 10, fun = mean)

# 4. FiveThirtyEight keeps a great archive of poll data at https://projects.fivethirtyeight.com/polls/. 
  # The presidential general election polling data is freely available at https://projects.fivethirtyeight.com/polls-page/president_polls.csv with question, poll id, and cycle defining a unique poll.
# 4a. Download and look at the data. Is it long or wide?

# Load data
getwd()
pres <- read.csv("homework/data/week_4/president_polls.csv")

# Look at data
head(pres)

# This is long data
   
#   4b. Get just the polling data for this last week (from 9/29 to today). 
  # Filter on start_date. Also filter down to just Biden and Trump (see candidate_name or answer). 
  # Extra credit for using {lubridate} for this, but you can just do a messy %in% string match.
library(lubridate)

# Change from factor to character
pres$start_date <- as.character(pres$start_date)

# Change from character to date
pres$start_date <- as.Date(pres$start_date, format = "%m/%d/%y")

# pres$start_date <- pres$start_date %>%
#   dplyr::filter(between(start_date, as.Date("2020-09-29"), as.Date("2020-10-05")))
  
# Filter to 9/29-10/05
pres_new <- pres[pres$start_date >= "2020-09-29" & pres$start_date <= "2020-10-05",]

# Filter to just Biden and Trump
pres_new <- pres_new %>%
 filter(candidate_name == 'Joseph R. Biden Jr.' |
          candidate_name =='Donald Trump')

# check pres_new
unique(pres_new$candidate_name)

# 4c. OK, this is your sample. What’s the bootstrapped average percentage for each candidate for nationwide polls (state == "")? 
  # Note, this answer will not match 538 given their weighting by poll trustworthiness.

# set seed
set.seed(123)

boot_cands <- pres_new %>%

  # group_by the candidate name  
group_by(candidate_name) %>%
  
  # filter by state = "" ??
  filter(state == "") %>%

  # summarize
  summarize(boot_pcts = replicate(100,
                                # bootstrap draws
                                sample(pres_new$pct,
                             size = length(pres_new),
                             replace = TRUE)) %>%
            mean())

# Don't need this since I piped it to mean above:
# get avg per candidate from the bootstrapped samples
#boot_avg_p <- boot_cands %>%
  #group_by(candidate_name) %>%
  #summarize(avg_pct = mean(boot_pcts))

# view it
boot_cands

# 4d. What is the average difference between the two candidates by state and national polls? 
  # Note, you’ll need to make this a wide data frame to answer! And, well, try the pivot without this advice first, but then….

# Try to pivot...
pres_wide <- pres_new %>%
  pivot_wider(names_from = candidate_name,
              values_from = pct)

# Create unique ID, starting over again with pres_new
pres_test <- pres_new %>%
  dplyr::mutate(unique_id = paste(question_id, poll_id, state, sep = "_"))

# Then select the ID, state, answer, and pct. Also filter out NA diffs
# now using pres_test
pres_test <- pres_test %>%
  select(unique_id, state, answer, pct)

# try pivoting:
pres_test_wide2 <- pres_test %>%
  pivot_wider(names_from = answer,
              values_from = pct)

# Get average by statewide polls & by nationwide polls
pres_test_wide2<- pres_test_wide2 %>%
  group_by(unique_id) %>%
  mutate(diff = abs(Biden - Trump)) %>%
  filter(!is.na(diff))
# This gives us the difference between each candidate by unique id


# average diff for each state
pres_test_wide3 <- pres_test_wide2 %>%
  # group by state polls
  group_by(state) %>%
  # get average difference of state polls
  mutate(avg_diff = mean(diff))

# Show this average difference:
final_answer <- pres_test_wide3 %>%
  select(state, avg_diff)

unique(final_answer)


# 5. replicate() has been our friend, but we’ve always had to be a little hacky with it. We’ve either had to fold in means, or use tricksy functions like colMeans and the like.
# BUT - what’s interesting about replicate() is that, if you ask it to turn back raw draws from a random number generator - or anything with more than one value - it gives you a matrix or array.
 # 5a. So, I want you to, using the mean and SD of Biden’s national polling average (you’ll need to calculate it!) from above, 

biden_national <- pres_test_wide3 %>%
              filter(state == "") %>%
               mutate(biden_national_mean = mean(Biden),
                      biden_national_sd= sd(Biden))

# Create mean object to use for next step  
mean <- unique(biden_national$biden_national_mean)
# Mean is  49.075

# Create sd object to use for next step
sd <- unique(biden_national$biden_national_sd)
# SD is 2.789148

  # simulate 1000 draws from that population with a sample size of 50. 

# Create the population, sample size of 50, mean and sd from previous question
pop <- rnorm(n = 50, mean = mean, sd = sd)

# Simulate 1000 draws:
biden_pop_draws <- replicate(1000, #1000 draws from...
          sample(pop, # the population
                 size = length(pop), # sample size the size of the population
                 replace = TRUE))

  # What are the dimensions of the object. What are in the rows and columns?
biden_pop_draws
# The dimensions are 1:50 x 1:1000 (50 rows, 1000 columns-- which are the 1000 simulations)
# The rows are the sample sizes, the columns are each draw of the 1000?

#   5b. Yuck. Can you turn this into something usable? 

    # Say, first make it a tibble or data frame, 

biden_pop_draws_df <- as.data.frame(biden_pop_draws)

    # and then pivot it to long, such that you end up with a column that has an 
      # identifier for sim and a column with a single value from that sim?
    #   (Oh, and for all columns, cols = everything())

biden_pop_draws_long <- biden_pop_draws_df %>%
  pivot_longer(cols = everything(),
               names_to = "sim",
               values_to = "percentage")

# Look at this new df
biden_pop_draws_long
 
# 5c. For each sim, what’s the bootstrapped mean and CI?

biden_boot_stats <- biden_pop_draws_long %>%
  # group by sim 
  group_by(sim) %>%
  # mutate it to get mean of each sim # 
  summarize(avg = mean(percentage),
         # get CIs
         ci_upper = quantile(percentage, probs = 0.975),
         ci_lower = quantile(percentage, probs = 0.025))
  
# Plot it! E.C. for the plot showing the stats in order from low to high. 

ggplot(data = biden_boot_stats,
       mapping = aes(x = sim,
                     y = avg,
                     ymin = ci_lower,
                     ymax = ci_upper)) +
  geom_pointrange(alpha = 0.3)


# And tell us how often it’s greater than the initial mean. 
# The error bars show that it's almost always greater than the initial mean.

# 5d. So…. what is that plot showing? What are the concepts involved?
# This CIs are very large, and expand far beyond the initial mean value of the data. 
  # These error bars reflect how precise the data is, i.e. the uncertainty.
#   EC 3 bonus point for each awesome quality visualization of the general polling data. There is a LOT there, so look carefully before you leap.